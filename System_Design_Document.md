# ğŸ—ï¸ AI ë°ì´í„° ë¶„ì„ ë¹„ì„œ - ì‹œìŠ¤í…œ ì„¤ê³„ ë¬¸ì„œ

<div align="center">

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Status](https://img.shields.io/badge/status-Design-yellow.svg)
![Architecture](https://img.shields.io/badge/architecture-Gradio%20%2B%20LangChain-green.svg)

---

**ğŸ“‹ ê¸°ë°˜ ë¬¸ì„œ**: [PRD_LLM_Data_Analysis_Service.md](./PRD_LLM_Data_Analysis_Service.md)  
**ğŸ¨ UI ì™€ì´ì–´í”„ë ˆì„**: [gradio_ui_wireframe.svg](./gradio_ui_wireframe.svg)

</div>

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

### ğŸ¯ ëª©ì 
ì´ ë¬¸ì„œëŠ” **AI ë°ì´í„° ë¶„ì„ ë¹„ì„œ** ì‹œìŠ¤í…œì˜ ê¸°ìˆ ì  ì„¤ê³„ë¥¼ ìƒì„¸íˆ ì •ì˜í•˜ì—¬, ê°œë°œíŒ€ì´ ì¼ê´€ëœ ì•„í‚¤í…ì²˜ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ“š ë¬¸ì„œ ë²”ìœ„
- ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„
- ì»´í¬ë„ŒíŠ¸ ë° ëª¨ë“ˆ ì„¤ê³„  
- API ì¸í„°í˜ì´ìŠ¤ ì •ì˜
- ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„
- ë³´ì•ˆ ë° ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ğŸ¯ ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

```mermaid
graph TB
    subgraph "ğŸŒ Client Layer"
        UI[Gradio Web UI]
        Mobile[Mobile Browser]
    end
    
    subgraph "ğŸ¨ Presentation Layer"
        GradioApp[Gradio Application]
        ChatInterface[Chat Interface]
        FileUpload[File Upload Handler]
        ChartRenderer[Chart Renderer]
    end
    
    subgraph "ğŸ§  Business Logic Layer"
        QueryProcessor[Query Processor]
        LangChainAgent[LangChain Agent]
        DataAnalyzer[Data Analyzer]
        ChartGenerator[Chart Generator]
    end
    
    subgraph "ğŸ”§ Service Layer"
        NLPService[NLP Service]
        SQLService[SQL Service]
        FileService[File Service]
        CacheService[Cache Service]
    end
    
    subgraph "ğŸ“Š Data Access Layer"
        DatabaseORM[SQLAlchemy ORM]
        FileManager[File Manager]
        APIClient[OpenAI API Client]
    end
    
    subgraph "ğŸ—„ï¸ Storage Layer"
        PostgreSQL[(PostgreSQL)]
        SQLite[(SQLite)]
        FileStorage[File Storage]
        Redis[(Redis Cache)]
    end
    
    subgraph "ğŸŒ External Services"
        OpenAI[OpenAI GPT-4 API]
        PlotlyService[Plotly Service]
    end
    
    UI --> GradioApp
    Mobile --> GradioApp
    GradioApp --> QueryProcessor
    GradioApp --> FileUpload
    GradioApp --> ChartRenderer
    
    QueryProcessor --> LangChainAgent
    QueryProcessor --> DataAnalyzer
    LangChainAgent --> NLPService
    DataAnalyzer --> SQLService
    DataAnalyzer --> FileService
    ChartGenerator --> ChartRenderer
    
    NLPService --> APIClient
    SQLService --> DatabaseORM
    FileService --> FileManager
    CacheService --> Redis
    
    DatabaseORM --> PostgreSQL
    DatabaseORM --> SQLite
    FileManager --> FileStorage
    APIClient --> OpenAI
    ChartRenderer --> PlotlyService
```

### ğŸ¯ ë ˆì´ì–´ë³„ ì±…ì„

| ë ˆì´ì–´ | ì£¼ìš” ì±…ì„ | ê¸°ìˆ  ìŠ¤íƒ |
|--------|-----------|-----------|
| **ğŸŒ Client** | ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì œê³µ | HTML5, CSS3, JavaScript |
| **ğŸ¨ Presentation** | UI ì»´í¬ë„ŒíŠ¸ ê´€ë¦¬ | Gradio 4.0+, Plotly |
| **ğŸ§  Business Logic** | í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬ | Python, LangChain |
| **ğŸ”§ Service** | ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™ | FastAPI, Pandas |
| **ğŸ“Š Data Access** | ë°ì´í„° ì ‘ê·¼ ì¶”ìƒí™” | SQLAlchemy, File I/O |
| **ğŸ—„ï¸ Storage** | ë°ì´í„° ì €ì¥ ë° ê´€ë¦¬ | PostgreSQL, Redis |

---

## ğŸ§© ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

### 1. ğŸ¤– Query Processor (ì§ˆì˜ ì²˜ë¦¬ê¸°)

```python
class QueryProcessor:
    """ìì—°ì–´ ì§ˆì˜ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œë¡œ ë¼ìš°íŒ…"""
    
    def __init__(self):
        self.langchain_agent = LangChainAgent()
        self.query_classifier = QueryClassifier()
        self.cache_service = CacheService()
    
    async def process_query(self, query: str, context: dict) -> QueryResult:
        """
        ì§ˆì˜ ì²˜ë¦¬ ë©”ì¸ í”Œë¡œìš°
        
        Args:
            query: ì‚¬ìš©ì ìì—°ì–´ ì§ˆì˜
            context: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ (ì—…ë¡œë“œëœ íŒŒì¼, DB ì—°ê²° ë“±)
            
        Returns:
            QueryResult: ì²˜ë¦¬ ê²°ê³¼ (í…ìŠ¤íŠ¸, ë°ì´í„°, ì°¨íŠ¸)
        """
        # 1. ìºì‹œ í™•ì¸
        cached_result = await self.cache_service.get(query)
        if cached_result:
            return cached_result
            
        # 2. ì§ˆì˜ ë¶„ë¥˜ (DB vs File vs Mixed)
        query_type = self.query_classifier.classify(query, context)
        
        # 3. ì ì ˆí•œ ì²˜ë¦¬ê¸°ë¡œ ë¼ìš°íŒ…
        if query_type == QueryType.DATABASE:
            result = await self._process_db_query(query, context)
        elif query_type == QueryType.FILE:
            result = await self._process_file_query(query, context)
        else:
            result = await self._process_mixed_query(query, context)
            
        # 4. ê²°ê³¼ ìºì‹±
        await self.cache_service.set(query, result)
        
        return result
```

### 2. ğŸ§  LangChain Agent (AI ì—ì´ì „íŠ¸)

```python
class LangChainAgent:
    """LangChain ê¸°ë°˜ AI ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)
        self.sql_chain = SQLDatabaseChain.from_llm(self.llm)
        self.pandas_agent = create_pandas_dataframe_agent(self.llm)
        
    async def generate_sql(self, query: str, schema: dict) -> str:
        """ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜"""
        prompt = self._build_sql_prompt(query, schema)
        sql = await self.sql_chain.arun(prompt)
        return self._validate_sql(sql)
        
    async def analyze_dataframe(self, df: pd.DataFrame, query: str) -> dict:
        """DataFrame ë¶„ì„ ìˆ˜í–‰"""
        analysis_result = await self.pandas_agent.arun(
            f"Analyze this data and answer: {query}"
        )
        return self._parse_analysis_result(analysis_result)
```

### 3. ğŸ“Š Data Analyzer (ë°ì´í„° ë¶„ì„ê¸°)

```python
class DataAnalyzer:
    """ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    
    def __init__(self):
        self.sql_service = SQLService()
        self.file_service = FileService()
        self.chart_generator = ChartGenerator()
        
    async def analyze_sql_data(self, sql: str, connection: str) -> AnalysisResult:
        """SQL ì¿¼ë¦¬ ì‹¤í–‰ ë° ë¶„ì„"""
        # 1. ì•ˆì „í•œ SQL ì‹¤í–‰
        df = await self.sql_service.execute_query(sql, connection)
        
        # 2. ê¸°ë³¸ í†µê³„ ìƒì„±
        stats = self._generate_statistics(df)
        
        # 3. ì ì ˆí•œ ì°¨íŠ¸ ì„ íƒ ë° ìƒì„±
        chart = await self.chart_generator.create_chart(df, stats)
        
        # 4. ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self._generate_insights(df, stats)
        
        return AnalysisResult(
            data=df,
            statistics=stats,
            chart=chart,
            insights=insights
        )
        
    def _generate_statistics(self, df: pd.DataFrame) -> dict:
        """ê¸°ìˆ í†µê³„ ìƒì„±"""
        return {
            'shape': df.shape,
            'dtypes': df.dtypes.to_dict(),
            'describe': df.describe().to_dict(),
            'null_counts': df.isnull().sum().to_dict(),
            'correlations': df.corr().to_dict() if len(df.select_dtypes(include=[np.number]).columns) > 1 else None
        }
```

### 4. ğŸ“ˆ Chart Generator (ì°¨íŠ¸ ìƒì„±ê¸°)

```python
class ChartGenerator:
    """ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ì°¨íŠ¸ ìë™ ìƒì„±"""
    
    def __init__(self):
        self.chart_rules = ChartSelectionRules()
        
    async def create_chart(self, df: pd.DataFrame, stats: dict) -> dict:
        """ìµœì  ì°¨íŠ¸ ìë™ ì„ íƒ ë° ìƒì„±"""
        # 1. ë°ì´í„° íŠ¹ì„± ë¶„ì„
        data_profile = self._analyze_data_profile(df, stats)
        
        # 2. ì°¨íŠ¸ íƒ€ì… ì„ íƒ
        chart_type = self.chart_rules.select_chart_type(data_profile)
        
        # 3. ì°¨íŠ¸ ìƒì„±
        chart_config = self._generate_chart_config(df, chart_type, data_profile)
        
        return {
            'type': chart_type,
            'config': chart_config,
            'plotly_json': self._create_plotly_chart(chart_config)
        }
        
    def _analyze_data_profile(self, df: pd.DataFrame, stats: dict) -> DataProfile:
        """ë°ì´í„° í”„ë¡œíŒŒì¼ ë¶„ì„"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
        
        return DataProfile(
            row_count=len(df),
            numeric_columns=numeric_cols,
            categorical_columns=categorical_cols,
            datetime_columns=datetime_cols,
            has_time_series=len(datetime_cols) > 0,
            cardinality={col: df[col].nunique() for col in categorical_cols}
        )
```

---

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„

### ğŸ“‹ ERD (Entity Relationship Diagram)

```mermaid
erDiagram
    USER {
        string user_id PK
        string username
        string email
        datetime created_at
        datetime last_login
        json preferences
    }
    
    SESSION {
        string session_id PK
        string user_id FK
        datetime created_at
        datetime expires_at
        json context
        boolean is_active
    }
    
    QUERY_HISTORY {
        string query_id PK
        string session_id FK
        string user_query
        string query_type
        json sql_generated
        json result_data
        json chart_config
        datetime executed_at
        float execution_time
        boolean is_successful
    }
    
    UPLOADED_FILE {
        string file_id PK
        string session_id FK
        string original_filename
        string stored_filename
        string file_type
        integer file_size
        string file_hash
        datetime uploaded_at
        datetime expires_at
        json metadata
    }
    
    DATABASE_CONNECTION {
        string connection_id PK
        string user_id FK
        string connection_name
        string db_type
        string host
        integer port
        string database_name
        string username
        string encrypted_password
        json schema_cache
        datetime created_at
        datetime last_used
        boolean is_active
    }
    
    CACHE_ENTRY {
        string cache_key PK
        string query_hash
        json cached_result
        datetime created_at
        datetime expires_at
        integer hit_count
    }
    
    USER ||--o{ SESSION : has
    SESSION ||--o{ QUERY_HISTORY : contains
    SESSION ||--o{ UPLOADED_FILE : uploads
    USER ||--o{ DATABASE_CONNECTION : owns
    QUERY_HISTORY }o--|| CACHE_ENTRY : cached_as
```

### ğŸ—ï¸ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ

#### 1. Users í…Œì´ë¸”
```sql
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP,
    preferences JSONB DEFAULT '{}',
    
    INDEX idx_username (username),
    INDEX idx_email (email)
);
```

#### 2. Sessions í…Œì´ë¸”
```sql
CREATE TABLE sessions (
    session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(user_id) ON DELETE CASCADE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NOT NULL,
    context JSONB DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    
    INDEX idx_user_id (user_id),
    INDEX idx_expires_at (expires_at),
    INDEX idx_is_active (is_active)
);
```

#### 3. Query History í…Œì´ë¸”
```sql
CREATE TABLE query_history (
    query_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES sessions(session_id) ON DELETE CASCADE,
    user_query TEXT NOT NULL,
    query_type VARCHAR(20) NOT NULL CHECK (query_type IN ('database', 'file', 'mixed')),
    sql_generated JSONB,
    result_data JSONB,
    chart_config JSONB,
    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    execution_time FLOAT,
    is_successful BOOLEAN NOT NULL,
    
    INDEX idx_session_id (session_id),
    INDEX idx_executed_at (executed_at),
    INDEX idx_query_type (query_type),
    INDEX idx_is_successful (is_successful)
);
```

---

## ğŸ”Œ API ì„¤ê³„

### ğŸ¯ RESTful API ì—”ë“œí¬ì¸íŠ¸

#### 1. ì§ˆì˜ ì²˜ë¦¬ API

```python
# POST /api/v1/query
{
    "query": "ì§€ë‚œ 3ê°œì›” ë§¤ì¶œ ì¶”ì´ë¥¼ ë³´ì—¬ì¤˜",
    "session_id": "uuid",
    "context": {
        "uploaded_files": ["file_id_1"],
        "database_connection": "connection_id_1"
    }
}

# Response
{
    "query_id": "uuid",
    "status": "success",
    "execution_time": 2.34,
    "result": {
        "text_response": "ì§€ë‚œ 3ê°œì›” ë§¤ì¶œì€ ê¾¸ì¤€íˆ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤.",
        "data": {
            "columns": ["month", "revenue"],
            "rows": [["2024-01", 1000000], ["2024-02", 1200000], ["2024-03", 1500000]]
        },
        "chart": {
            "type": "line",
            "plotly_json": {...}
        },
        "insights": [
            "ë§¤ì›” í‰ê·  20% ì„±ì¥ë¥ ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.",
            "3ì›”ì— ê°€ì¥ ë†’ì€ ë§¤ì¶œì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."
        ]
    }
}
```

#### 2. íŒŒì¼ ì—…ë¡œë“œ API

```python
# POST /api/v1/files/upload
# Content-Type: multipart/form-data
{
    "file": <binary_data>,
    "session_id": "uuid"
}

# Response
{
    "file_id": "uuid",
    "original_filename": "sales_data.xlsx",
    "file_size": 1024000,
    "columns": ["date", "product", "revenue", "quantity"],
    "row_count": 10000,
    "preview": [
        {"date": "2024-01-01", "product": "A", "revenue": 1000, "quantity": 10},
        {"date": "2024-01-02", "product": "B", "revenue": 1500, "quantity": 15}
    ]
}
```

#### 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° API

```python
# POST /api/v1/database/connect
{
    "connection_name": "Production DB",
    "db_type": "postgresql",
    "host": "localhost",
    "port": 5432,
    "database": "sales_db",
    "username": "analyst",
    "password": "encrypted_password"
}

# Response
{
    "connection_id": "uuid",
    "status": "connected",
    "schema": {
        "tables": [
            {
                "name": "sales",
                "columns": [
                    {"name": "id", "type": "integer", "nullable": false},
                    {"name": "date", "type": "date", "nullable": false},
                    {"name": "amount", "type": "decimal", "nullable": false}
                ]
            }
        ]
    }
}
```

### ğŸ”„ WebSocket API (ì‹¤ì‹œê°„ ì²˜ë¦¬)

```python
# WebSocket /ws/query/{session_id}

# Client -> Server
{
    "type": "query_start",
    "query": "ë³µì¡í•œ ë¶„ì„ ì§ˆì˜...",
    "context": {...}
}

# Server -> Client (Progress Updates)
{
    "type": "progress",
    "stage": "sql_generation",
    "progress": 25,
    "message": "SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
}

{
    "type": "progress", 
    "stage": "data_processing",
    "progress": 75,
    "message": "ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
}

# Server -> Client (Final Result)
{
    "type": "query_complete",
    "result": {...}
}
```

---

## ğŸ›¡ï¸ ë³´ì•ˆ ì„¤ê³„

### ğŸ” ì¸ì¦ ë° ê¶Œí•œ ê´€ë¦¬

```python
class SecurityManager:
    """ë³´ì•ˆ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.jwt_secret = os.getenv('JWT_SECRET')
        self.password_hasher = PasswordHasher()
        
    def authenticate_user(self, username: str, password: str) -> Optional[User]:
        """ì‚¬ìš©ì ì¸ì¦"""
        user = self.user_repository.get_by_username(username)
        if user and self.password_hasher.verify(password, user.password_hash):
            return user
        return None
        
    def generate_session_token(self, user: User) -> str:
        """ì„¸ì…˜ í† í° ìƒì„±"""
        payload = {
            'user_id': str(user.user_id),
            'username': user.username,
            'exp': datetime.utcnow() + timedelta(hours=24)
        }
        return jwt.encode(payload, self.jwt_secret, algorithm='HS256')
```

### ğŸ›¡ï¸ SQL Injection ë°©ì§€

```python
class SQLSecurityValidator:
    """SQL ë³´ì•ˆ ê²€ì¦ê¸°"""
    
    FORBIDDEN_KEYWORDS = [
        'DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE',
        'TRUNCATE', 'EXEC', 'EXECUTE', 'GRANT', 'REVOKE'
    ]
    
    def validate_sql(self, sql: str) -> bool:
        """SQL ì•ˆì „ì„± ê²€ì¦"""
        # 1. ê¸ˆì§€ëœ í‚¤ì›Œë“œ í™•ì¸
        sql_upper = sql.upper()
        for keyword in self.FORBIDDEN_KEYWORDS:
            if keyword in sql_upper:
                raise SecurityError(f"Forbidden keyword detected: {keyword}")
                
        # 2. SELECT ë¬¸ë§Œ í—ˆìš©
        if not sql_upper.strip().startswith('SELECT'):
            raise SecurityError("Only SELECT statements are allowed")
            
        # 3. íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ ê°•ì œ
        if ';' in sql and sql.count(';') > 1:
            raise SecurityError("Multiple statements not allowed")
            
        return True
```

### ğŸ”’ ë°ì´í„° ì•”í˜¸í™”

```python
class DataEncryption:
    """ë°ì´í„° ì•”í˜¸í™” ê´€ë¦¬"""
    
    def __init__(self):
        self.fernet = Fernet(os.getenv('ENCRYPTION_KEY'))
        
    def encrypt_sensitive_data(self, data: str) -> str:
        """ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™” (DB ë¹„ë°€ë²ˆí˜¸ ë“±)"""
        return self.fernet.encrypt(data.encode()).decode()
        
    def decrypt_sensitive_data(self, encrypted_data: str) -> str:
        """ì•”í˜¸í™”ëœ ë°ì´í„° ë³µí˜¸í™”"""
        return self.fernet.decrypt(encrypted_data.encode()).decode()
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### ğŸš€ ìºì‹± ì „ëµ

```python
class CacheService:
    """ë‹¤ì¸µ ìºì‹± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        self.memory_cache = {}
        
    async def get_query_result(self, query_hash: str) -> Optional[dict]:
        """ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ ì¡°íšŒ"""
        # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸ (ê°€ì¥ ë¹ ë¦„)
        if query_hash in self.memory_cache:
            return self.memory_cache[query_hash]
            
        # 2. Redis ìºì‹œ í™•ì¸
        cached_data = await self.redis_client.get(f"query:{query_hash}")
        if cached_data:
            result = json.loads(cached_data)
            # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
            self.memory_cache[query_hash] = result
            return result
            
        return None
        
    async def cache_query_result(self, query_hash: str, result: dict, ttl: int = 3600):
        """ì¿¼ë¦¬ ê²°ê³¼ ìºì‹±"""
        # Redisì— ì €ì¥ (TTL ì ìš©)
        await self.redis_client.setex(
            f"query:{query_hash}", 
            ttl, 
            json.dumps(result)
        )
        
        # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥ (í¬ê¸° ì œí•œ)
        if len(self.memory_cache) < 100:
            self.memory_cache[query_hash] = result
```

### ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ìµœì í™”

```python
class DataProcessor:
    """ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ìµœì í™”"""
    
    def __init__(self):
        self.chunk_size = 10000
        self.max_memory_usage = 500 * 1024 * 1024  # 500MB
        
    async def process_large_dataset(self, df: pd.DataFrame) -> dict:
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        if len(df) > self.chunk_size:
            return await self._process_in_chunks(df)
        else:
            return await self._process_full_dataset(df)
            
    async def _process_in_chunks(self, df: pd.DataFrame) -> dict:
        """ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬"""
        results = []
        
        for chunk_start in range(0, len(df), self.chunk_size):
            chunk_end = min(chunk_start + self.chunk_size, len(df))
            chunk = df.iloc[chunk_start:chunk_end]
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            if self._get_memory_usage() > self.max_memory_usage:
                gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                
            chunk_result = await self._analyze_chunk(chunk)
            results.append(chunk_result)
            
        return self._merge_chunk_results(results)
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        
    @contextmanager
    def measure_execution_time(self, operation_name: str):
        """ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        try:
            yield
        finally:
            execution_time = time.time() - start_time
            self.metrics_collector.record_execution_time(
                operation_name, 
                execution_time
            )
            
    def record_query_metrics(self, query_type: str, success: bool, execution_time: float):
        """ì¿¼ë¦¬ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics_collector.increment_counter(
            f"queries.{query_type}.{'success' if success else 'failure'}"
        )
        self.metrics_collector.record_histogram(
            f"query_execution_time.{query_type}",
            execution_time
        )
```

### ğŸ“ êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
import structlog

class LoggingConfig:
    """ë¡œê¹… ì„¤ì •"""
    
    @staticmethod
    def setup_logging():
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

# ì‚¬ìš© ì˜ˆì‹œ
logger = structlog.get_logger()

async def process_query(query: str, session_id: str):
    logger.info(
        "Query processing started",
        query=query,
        session_id=session_id,
        user_id=current_user.id
    )
    
    try:
        result = await query_processor.process(query)
        logger.info(
            "Query processing completed",
            query=query,
            session_id=session_id,
            execution_time=result.execution_time,
            result_rows=len(result.data)
        )
        return result
    except Exception as e:
        logger.error(
            "Query processing failed",
            query=query,
            session_id=session_id,
            error=str(e),
            exc_info=True
        )
        raise
```

---

## ğŸš€ ë°°í¬ ì•„í‚¤í…ì²˜

### ğŸ³ Docker ì»¨í…Œì´ë„ˆ êµ¬ì„±

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "7860:7860"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/aianalyst
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - redis
    volumes:
      - ./uploads:/app/uploads
      
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=aianalyst
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      
  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
      
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - app

volumes:
  postgres_data:
  redis_data:
```

### â˜¸ï¸ Kubernetes ë°°í¬ (ì„ íƒì‚¬í•­)

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-analyst-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-analyst
  template:
    metadata:
      labels:
        app: ai-analyst
    spec:
      containers:
      - name: app
        image: ai-analyst:latest
        ports:
        - containerPort: 7860
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
```

---

## ğŸ“‹ ê°œë°œ ê°€ì´ë“œë¼ì¸

### ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ai-data-analyst/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Gradio ì•± ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py         # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ database.py         # DB ì—°ê²° ì„¤ì •
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ query_processor.py  # ì§ˆì˜ ì²˜ë¦¬ê¸°
â”‚   â”‚   â”œâ”€â”€ langchain_agent.py  # LangChain ì—ì´ì „íŠ¸
â”‚   â”‚   â””â”€â”€ data_analyzer.py    # ë°ì´í„° ë¶„ì„ê¸°
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ nlp_service.py      # NLP ì„œë¹„ìŠ¤
â”‚   â”‚   â”œâ”€â”€ sql_service.py      # SQL ì„œë¹„ìŠ¤
â”‚   â”‚   â”œâ”€â”€ file_service.py     # íŒŒì¼ ì„œë¹„ìŠ¤
â”‚   â”‚   â””â”€â”€ cache_service.py    # ìºì‹œ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py         # SQLAlchemy ëª¨ë¸
â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ security.py         # ë³´ì•ˆ ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ validators.py       # ê²€ì¦ ë¡œì§
â”‚   â”‚   â””â”€â”€ helpers.py          # í—¬í¼ í•¨ìˆ˜
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ components.py       # Gradio ì»´í¬ë„ŒíŠ¸
â”‚       â””â”€â”€ layouts.py          # UI ë ˆì´ì•„ì›ƒ
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_query_processor.py
â”‚   â”œâ”€â”€ test_data_analyzer.py
â”‚   â””â”€â”€ test_services.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ api.md
â”‚   â”œâ”€â”€ deployment.md
â”‚   â””â”€â”€ user_guide.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_db.py
â”‚   â””â”€â”€ migrate_data.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

### ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

```python
# tests/test_query_processor.py
import pytest
from app.core.query_processor import QueryProcessor

class TestQueryProcessor:
    
    @pytest.fixture
    def query_processor(self):
        return QueryProcessor()
        
    @pytest.mark.asyncio
    async def test_process_db_query(self, query_processor):
        """DB ì§ˆì˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        query = "Show me sales data for last month"
        context = {"database_connection": "test_db"}
        
        result = await query_processor.process_query(query, context)
        
        assert result.status == "success"
        assert result.query_type == "database"
        assert result.data is not None
        
    @pytest.mark.asyncio
    async def test_process_file_query(self, query_processor):
        """íŒŒì¼ ì§ˆì˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        query = "Analyze the uploaded sales data"
        context = {"uploaded_files": ["test_file.xlsx"]}
        
        result = await query_processor.process_query(query, context)
        
        assert result.status == "success"
        assert result.query_type == "file"
```

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

### ğŸ”— ê´€ë ¨ ë¬¸ì„œ
- [PRD_LLM_Data_Analysis_Service.md](./PRD_LLM_Data_Analysis_Service.md) - ì œí’ˆ ìš”êµ¬ì‚¬í•­ ë¬¸ì„œ
- [gradio_ui_wireframe.svg](./gradio_ui_wireframe.svg) - UI ì™€ì´ì–´í”„ë ˆì„
- [wireframe_guide.md](./wireframe_guide.md) - UI êµ¬í˜„ ê°€ì´ë“œ

### ğŸ“– ê¸°ìˆ  ë¬¸ì„œ
- [Gradio Documentation](https://gradio.app/docs/)
- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Plotly Documentation](https://plotly.com/python/)

---

<div align="center">

**ğŸ“… ë¬¸ì„œ ë²„ì „**: v1.0.0  
**ğŸ“ ìµœì¢… ìˆ˜ì •**: 2024ë…„ 1ì›”  
**ğŸ‘¥ ì‘ì„±ì**: AI ë°ì´í„° ë¶„ì„ ë¹„ì„œ ê°œë°œíŒ€

---

*ì´ ì„¤ê³„ ë¬¸ì„œëŠ” PRDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ê°œë°œ ê³¼ì •ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.*

</div>
